{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# VGG"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "name = 'vgg'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import needed libraries\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import utilities\n",
    "from tensorflow.keras.layers import Input, Conv2D, Dense, Flatten, Dropout, MaxPooling2D, BatchNormalization\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check Tensorflow version\n",
    "print('Tensorflow version: ',tf.__version__)\n",
    "# Check if GPU is being used\n",
    "print('GPU available: ', tf.config.list_physical_devices('GPU'))\n",
    "# If a GPU is being used, check that it's not being used by another process as well\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "  print(\"GPU usage: \", tf.config.experimental.get_memory_info('GPU:0'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load the data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def process_image(path, label):\n",
    "    # Desired size\n",
    "    size = 250\n",
    "    # Get the image\n",
    "    img = tf.io.read_file(path)\n",
    "    # Decode the PNG\n",
    "    img = tf.image.decode_png(img)\n",
    "    # Resize image\n",
    "    img = tf.image.resize(img, (size, size))\n",
    "    # Reshape image (this is not necessary but I do it so that I don't need to be modifying the shape in the input layer)\n",
    "    img = tf.reshape(img, [size, size, 1])\n",
    "    # Cast image to float32\n",
    "    img = tf.cast(img, tf.float32)\n",
    "    # Normalize image\n",
    "    img = img/255.0\n",
    "\n",
    "    return img, label"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_paths_n_labels(csv_path, images_folder_path, id_col, label_col):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    paths = [images_folder_path + '/' + str(id) + '.png' for id in df[id_col].tolist()]\n",
    "    labels = df[label_col].tolist()\n",
    "    return paths, labels"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Dataset used\n",
    "ds = 'clahecropfill_r250'\n",
    "\n",
    "# Input pipeline\n",
    "# Training set\n",
    "train_paths, train_labels = get_paths_n_labels( csv_path='./data/pre_processed/training/train.csv',\n",
    "                                                images_folder_path='./data/pre_processed/training/{}'.format(ds),\n",
    "                                                id_col='id',\n",
    "                                                label_col='boneage')\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_paths, train_labels))\n",
    "train_dataset = train_dataset.map(process_image)\n",
    "train_dataset = train_dataset.shuffle(len(train_dataset)).batch(32)\n",
    "\n",
    "#Validation set\n",
    "validation_paths, validation_labels = get_paths_n_labels(\n",
    "                                                csv_path='./data/pre_processed/validation/validation_1/validation_1.csv',\n",
    "                                                images_folder_path='./data/pre_processed/validation/validation_1/{}'.format(ds),\n",
    "                                                id_col='Image ID',\n",
    "                                                label_col='Bone Age (months)')\n",
    "validation_dataset = tf.data.Dataset.from_tensor_slices((validation_paths, validation_labels))\n",
    "validation_dataset = validation_dataset.map(process_image).batch(32)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# for img, label in train_dataset:\n",
    "#     print(type(img))\n",
    "#     break"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Instantiate the model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Build the model using the functional API\n",
    "i = Input(shape=train_dataset.element_spec[0].shape[1:])\n",
    "x = Conv2D(8, (3,3), activation='relu', padding='same')(i)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(8, (3,3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D((2,2))(x)\n",
    "x = Conv2D(16, (3,3), activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(16, (3,3), activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2,2))(x)\n",
    "x = Conv2D(32, (3,3), strides=2, activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(32, (3,3), strides=2, activation='relu', padding='same' )(x)\n",
    "x = MaxPooling2D((2,2))(x)\n",
    "x = Flatten()(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(1)(x)\n",
    "\n",
    "model = Model(i,x)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "              loss='mse',\n",
    "              metrics=[tf.keras.metrics.RootMeanSquaredError()])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# To make that the learning rate decreases\n",
    "def schedule(epoch, lr):\n",
    "    if epoch>=50:\n",
    "        return 0.0001\n",
    "    return 0.001\n",
    "scheduler = tf.keras.callbacks.LearningRateScheduler(schedule)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Experiment tracking with tensorboard\n",
    "import time\n",
    "experiment_name = \"{}_{}_{}\".format(name,ds,int(time.time()))\n",
    "tensorboard = tf.keras.callbacks.TensorBoard(log_dir='./logs/{}'.format(experiment_name))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training (\"fit\") the model"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "InternalError",
     "evalue": "Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mInternalError\u001B[0m                             Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[13], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Fit the model\u001B[39;00m\n\u001B[0;32m      2\u001B[0m batch_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m3\u001B[39m  \u001B[38;5;66;03m# or any other integer value that works for your system\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m r \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalidation_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mx_validate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_validate\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m15\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\tensorflow_gpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[0;32m     68\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[0;32m     69\u001B[0m     \u001B[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001B[39;00m\n\u001B[1;32m---> 70\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[0;32m     71\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m     72\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\tensorflow_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:102\u001B[0m, in \u001B[0;36mconvert_to_eager_tensor\u001B[1;34m(value, ctx, dtype)\u001B[0m\n\u001B[0;32m    100\u001B[0m     dtype \u001B[38;5;241m=\u001B[39m dtypes\u001B[38;5;241m.\u001B[39mas_dtype(dtype)\u001B[38;5;241m.\u001B[39mas_datatype_enum\n\u001B[0;32m    101\u001B[0m ctx\u001B[38;5;241m.\u001B[39mensure_initialized()\n\u001B[1;32m--> 102\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mops\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mEagerTensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdevice_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mInternalError\u001B[0m: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "208/395 [==============>...............] - ETA: 22s - loss: 3861.5698 - root_mean_squared_error: 62.1415"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "r = model.fit(train_dataset, validation_data=validation_dataset, epochs=50, callbacks=[scheduler, tensorboard])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Save the model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Let's now save our model to a file\n",
    "model.save('./models/{}.h5'.format(experiment_name))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Make predictions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model = tf.keras.models.load_model('./models/cnn_1_clahecrop2_r250_1675982311.h5')\n",
    "model.evaluate(validation_dataset)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "p_test = model.predict(validation_dataset).reshape(-1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Show some misclassified examples\n",
    "y_test = np.array([y.numpy() for x, y in validation_dataset.unbatch()])\n",
    "x_test = np.array([x.numpy() for x, y in validation_dataset.unbatch()])\n",
    "difference = np.abs(np.subtract(y_test, p_test))\n",
    "misclassified_idx = np.where(difference<4)\n",
    "#print(misclassified_idx)\n",
    "i = np.random.choice(misclassified_idx[0])\n",
    "plt.imshow(x_test[i], cmap='gray')\n",
    "plt.title(\"True label: %s Predicted: %s\" % (y_test[i], p_test[i]));"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

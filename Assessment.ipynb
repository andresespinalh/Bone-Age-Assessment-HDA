{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Testing the models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version:  2.11.0\n"
     ]
    }
   ],
   "source": [
    "# Check Tensorflow version\n",
    "print('Tensorflow version: ',tf.__version__)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load the test data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def process_image_gender(path_gender, label):\n",
    "    # Desired size\n",
    "    size = 250\n",
    "    path, gender = path_gender\n",
    "    # Get the image\n",
    "    img = tf.io.read_file(path)\n",
    "    # Decode the PNG\n",
    "    img = tf.image.decode_png(img)\n",
    "    # Resize image\n",
    "    img = tf.image.resize(img, (size, size))\n",
    "    # Reshape image (this is not necessary but I do it so that I don't need to be modifying the shape in the input layer)\n",
    "    img = tf.reshape(img, [size, size, 1])\n",
    "    # Cast image to float32\n",
    "    img = tf.cast(img, tf.float32)\n",
    "    # Normalize image\n",
    "    img = img/255.0\n",
    "\n",
    "    return (img, gender), label\n",
    "def process_image(path, label):\n",
    "    # Desired size\n",
    "    size = 250\n",
    "    # Get the image\n",
    "    img = tf.io.read_file(path)\n",
    "    # Decode the PNG\n",
    "    img = tf.image.decode_png(img)\n",
    "    # Resize image\n",
    "    img = tf.image.resize(img, (size, size))\n",
    "    # Reshape image (this is not necessary but I do it so that I don't need to be modifying the shape in the input layer)\n",
    "    img = tf.reshape(img, [size, size, 1])\n",
    "    # Cast image to float32\n",
    "    img = tf.cast(img, tf.float32)\n",
    "    # Normalize image\n",
    "    img = img/255.0\n",
    "\n",
    "    return img, label\n",
    "def get_paths_n_labels(csv_path, images_folder_path, id_col, label_col):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    paths = [images_folder_path + '/' + str(id) + '.png' for id in df[id_col].tolist()]\n",
    "    labels = df[label_col].tolist()\n",
    "    return paths, labels\n",
    "def get_data(csv_path, images_folder_path, id_col, label_col, gender_col):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    paths = [images_folder_path + '/' + str(id) + '.png' for id in df[id_col].tolist()]\n",
    "    labels = df[label_col].tolist()\n",
    "    gender = [0 if (str(g)==\"False\" or str(g)==\"FALSE\" or str(g)==\"F\") else 1 for g in df[gender_col].tolist()] # male 1, female 0\n",
    "    return paths, labels, gender"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# original_r250p\n",
    "otest_paths, otest_labels, otest_gender = get_data(\n",
    "                                                csv_path='./data/pre_processed/test/test.csv',\n",
    "                                                images_folder_path='./data/pre_processed/test/{}'.format('original_r250p'),\n",
    "                                                id_col='Case ID',\n",
    "                                                label_col='Ground truth bone age (months)',\n",
    "                                                gender_col='Sex')\n",
    "# Without gender\n",
    "test_dataset_orig = tf.data.Dataset.from_tensor_slices((otest_paths,otest_labels)).map(process_image).batch(32)\n",
    "# With gender\n",
    "test_dataset_orig_gender = tf.data.Dataset.from_tensor_slices(((otest_paths,otest_gender),otest_labels)).map(process_image_gender).batch(32)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# preprocessed_r250p\n",
    "ptest_paths, ptest_labels, ptest_gender = get_data(\n",
    "                                                csv_path='./data/pre_processed/test/test.csv',\n",
    "                                                images_folder_path='./data/pre_processed/test/{}'.format('preprocessed_r250p'),\n",
    "                                                id_col='Case ID',\n",
    "                                                label_col='Ground truth bone age (months)',\n",
    "                                                gender_col='Sex')\n",
    "# Without gender\n",
    "test_dataset_preproc = tf.data.Dataset.from_tensor_slices((ptest_paths,ptest_labels)).map(process_image).batch(32)\n",
    "# With gender\n",
    "test_dataset_preproc_gender = tf.data.Dataset.from_tensor_slices(((ptest_paths,ptest_gender),ptest_labels)).map(process_image_gender).batch(32)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load the models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Load the models with gender\n",
    "inceptionv4_gender_orig = tf.keras.models.load_model('./models/inceptionv4_gender__original_r250p_1676203916.h5')\n",
    "inceptionv4_gender_preproc = tf.keras.models.load_model('./models/inceptionv4_gender__preprocessed_r250p_1676244085.h5')\n",
    "vgg6_gender_orig = tf.keras.models.load_model('./models/vgg_gender_original_r250p_1676150023.h5')\n",
    "vgg6_gender_preproc = tf.keras.models.load_model('./models/vgg_gender_preprocessed_r250p_1676154512.h5')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Load the models without gender\n",
    "inceptionv4_orig = tf.keras.models.load_model('./models/inceptionv4_original_r250p_1676207504.h5')\n",
    "inceptionv4_preproc = tf.keras.models.load_model('./models/inceptionv4_preprocessed_r250p_1676249796.h5')\n",
    "vgg6_orig = tf.keras.models.load_model('./models/vgg_original_r250p_1676147625.h5')\n",
    "vgg6_preproc = tf.keras.models.load_model('./models/vgg_preprocessed_r250p_1676153487.h5')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluate the models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Evaluate models\n",
    "def evaluate_model_gender(model, dataset):\n",
    "    p_test = model.predict(dataset).reshape(-1)\n",
    "    y_test = np.array([label.numpy() for (img,gender), label in dataset.unbatch()])\n",
    "    mae = np.mean(np.abs(np.subtract(y_test, p_test)))\n",
    "    print(\"Mean absolute error: \"+str(mae))\n",
    "def evaluate_model(model, dataset):\n",
    "    p_test = model.predict(dataset).reshape(-1)\n",
    "    y_test = np.array([label.numpy() for img, label in dataset.unbatch()])\n",
    "    mae = np.mean(np.abs(np.subtract(y_test, p_test)))\n",
    "    print(\"Mean absolute error: \"+str(mae))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### VGG-6"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "w/o Gender\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 1s 103ms/step\n",
      "Mean absolute error: 16.205986\n"
     ]
    }
   ],
   "source": [
    "# original_r250p\n",
    "evaluate_model(vgg6_orig,test_dataset_orig)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 1s 106ms/step\n",
      "Mean absolute error: 14.109846\n"
     ]
    }
   ],
   "source": [
    "# preprocessed_r250p\n",
    "evaluate_model(vgg6_preproc,test_dataset_preproc)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "w/ Gender"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 1s 109ms/step\n",
      "Mean absolute error: 15.715346\n"
     ]
    }
   ],
   "source": [
    "# original_r250p\n",
    "evaluate_model_gender(vgg6_gender_orig,test_dataset_orig_gender)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 1s 100ms/step\n",
      "Mean absolute error: 9.180414\n"
     ]
    }
   ],
   "source": [
    "# preprocessed_r250p\n",
    "evaluate_model_gender(vgg6_gender_preproc,test_dataset_preproc_gender)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Inception-V4"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "w/o Gender\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 12s 1s/step\n",
      "Mean absolute error: 10.930362\n"
     ]
    }
   ],
   "source": [
    "# original_r250p\n",
    "evaluate_model(inceptionv4_orig,test_dataset_orig)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 12s 2s/step\n",
      "Mean absolute error: 10.53585\n"
     ]
    }
   ],
   "source": [
    "# preprocessed_r250p\n",
    "evaluate_model(inceptionv4_preproc,test_dataset_preproc)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "w/ Gender"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 12s 1s/step\n",
      "Mean absolute error: 8.053529\n"
     ]
    }
   ],
   "source": [
    "# original_r250p\n",
    "evaluate_model_gender(inceptionv4_gender_orig,test_dataset_orig_gender)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 13s 2s/step\n",
      "Mean absolute error: 7.4948206\n"
     ]
    }
   ],
   "source": [
    "# preprocessed_r250p\n",
    "evaluate_model_gender(inceptionv4_gender_preproc,test_dataset_preproc_gender)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
